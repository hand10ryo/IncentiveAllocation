{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IncentiveAllocation import (\n",
    "    BinaryTreatmentPolicy as policy,\n",
    "    BinaryTreatmentLearner as lerner,\n",
    "    BinaryTreatmentEvaluator as evaluator,\n",
    "    BinaryTreatmentOptimizer as optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklift.datasets import fetch_hillstrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_hillstrom(target_col=\"spend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1 - (data[\"treatment\"] == \"No E-Mail\").astype(int)\n",
    "y = data[\"target\"]\n",
    "z = (data[\"target\"] > 0).astype(int) * t\n",
    "X = pd.DataFrame(OrdinalEncoder().fit_transform(data[\"data\"]), columns=data[\"data\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 51200, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 337.984290\n",
      "[LightGBM] [Info] Number of positive: 369, number of negative: 33877\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 34246, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010775 -> initscore=-4.519695\n",
      "[LightGBM] [Info] Start training from score -4.519695\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 51200, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 351.529464\n",
      "[LightGBM] [Info] Number of positive: 370, number of negative: 33732\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 34102, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010850 -> initscore=-4.512699\n",
      "[LightGBM] [Info] Start training from score -4.512699\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 51200, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 353.580010\n",
      "[LightGBM] [Info] Number of positive: 356, number of negative: 33796\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 34152, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010424 -> initscore=-4.553167\n",
      "[LightGBM] [Info] Start training from score -4.553167\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 51200, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 348.021780\n",
      "[LightGBM] [Info] Number of positive: 372, number of negative: 33821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 34193, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010879 -> initscore=-4.509943\n",
      "[LightGBM] [Info] Start training from score -4.509943\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 51200, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 308.446283\n",
      "[LightGBM] [Info] Number of positive: 357, number of negative: 33726\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 34083, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010474 -> initscore=-4.548289\n",
      "[LightGBM] [Info] Start training from score -4.548289\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "kf = KFold(n_splits=5, shuffle = True, random_state=1)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    z_train, z_test = z.iloc[train_index], z.iloc[test_index]\n",
    "    t_train, t_test = t.iloc[train_index], t.iloc[test_index]\n",
    "\n",
    "    revenue_model = LGBMRegressor()\n",
    "    cost_model = LGBMClassifier()\n",
    "    t_lerner = lerner.TOTLearner(revenue_model, cost_model)\n",
    "    greedy_optimizer = optimizer.GreedyOptimizer()\n",
    "    t_policy = policy.BinaryTreatmentPolicy(t_lerner, greedy_optimizer)\n",
    "\n",
    "    t_policy.fit(X_train, y_train, z_train, t_train)\n",
    "\n",
    "    aRoas = evaluator.AddonPerCost(t_policy, X_test, t_test, y_test, z_test)\n",
    "    score = aRoas.score(30)\n",
    "    score_list.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[193.1692727669797,\n",
       " 237.98758796829773,\n",
       " 133.67974140562984,\n",
       " 157.64285099670158,\n",
       " 67.72942504528979]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incentiveallocation-e2CN676e-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
